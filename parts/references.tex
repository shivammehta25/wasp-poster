\small
% \vspace{-0.5cm}
% \fcolorbox{white}{white}{\hspace{-0.5cm}\parbox{0.1\linewidth}{\centering [1]}\parbox{1.3cm}{\includegraphics[trim=28mm 10mm 35mm 15mm,clip,width=0.8cm]{qracm}}\parbox{0.7\linewidth}{\footnotesize\smaller Five Challenges in Cloud-Enabled Intelligence and Control\\\tiny T. Abdelzaher, Y. Hao, K. Jayarajah, A. Misra, S. Yao, P. Skarin, D. Weerakoon and K. {\AA}rz{\'e}n\\ACM Transactions on Internet Technology, 2019}}\par\vspace{-0.4cm}

\vspace{-0.5cm}
\fcolorbox{white}{white}{\hspace{-0.5cm}\parbox{0.1\linewidth}{\centering [1]}\parbox{0.9\linewidth}{\footnotesize\tiny T. Fujimoto, K. Hashimoto, Y. Nankaku, and K. Tokuda, “Autoregressive variational autoencoder with a hidden semi-Markov model-based structured attention for speech synthesis,” in Proc. ICASSP, 2022.}}\par

\fcolorbox{white}{white}{\hspace{-0.5cm}\parbox{0.1\linewidth}{\centering [2]}\parbox{0.9\linewidth}{\footnotesize \tiny O. Watts, G. E. Henter, J. Fong, and C. Valentini-Botinhao, “Where do the improvements come from in sequence-to-sequence neural TTS?,” in Proc. SSW, 2019.}}\par
	
\fcolorbox{white}{white}{\hspace{-0.5cm}\parbox{0.1\linewidth}{\centering [3]}\parbox{0.9\linewidth}{\footnotesize\tiny S. Mehta, É. Székely, J. Beskow, G. E. Henter, "Neural HMMs are all you need (for high-quality attention-free TTS)", in Proc. ICASSP2022}}\par